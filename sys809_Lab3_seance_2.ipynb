{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 Vision par ordinateur : Apprentissage profond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Mohsen Benlazreg M.ing en GPA\n",
    "\n",
    "Mise à jour: Étienne Pepin\n",
    "\n",
    "REF : https://keras.io/examples/\n",
    "#### requirements :\n",
    "numpy  \n",
    "sklearn  \n",
    "tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séance 2 : Utilisation des architectures pré-entraînées"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une approche courante et très efficace de l'apprentissage profond consiste à conserver une partie d'un réseau de neurones pré-entrainé. Un réseau pré-entrainé est simplement un réseau entrainé sur une grande base données, généralement sur une tâche de classification d'images à grande échelle.\n",
    "\n",
    "Si cette base de données utilisée est assez grande et assez générale, alors la hiérarchie des caractéristiques spatiales apprise par le réseau pré-entrainé peut effectivement agir comme un modèle générique de notre monde visuel, et donc ses caractéristiques peuvent s'avérer utiles pour de nombreux problèmes de vision par ordinateur différents, même si ces nouveaux problèmes puissent impliquer des classes complètement différentes.\n",
    "\n",
    "Dans notre cas, nous considérerons un grand convnet entrainé sur la base de données ImageNet (1,4 million d'images étiquetées et 1000 classes différentes).\n",
    "\n",
    "Nous utiliserons l'architecture VGG16, développée par Karen Simonyan et Andrew Zisserman en 2014, une architecture convnet simple et largement utilisée.\n",
    "\n",
    "C'est un modèle un peu plus ancien, loin de l'état de l'art actuel mais nous l'avons choisi car son architecture est facile à comprendre sans introduire de nouveaux concepts.\n",
    "\n",
    "Le modèle VGG16 est donné dans le framework Keras. Vous pouvez l'importer depuis le module `keras.applications`. Voici la liste quelques modèles pré-entrainé sur ImageNet fournit par Keras:\n",
    "* Xception\n",
    "* InceptionV3\n",
    "* ResNet50\n",
    "* VGG16\n",
    "* VGG19\n",
    "* MobileNet\n",
    "\n",
    "Il existe deux façons d'exploiter un réseau pré-entrainé :\n",
    "- Utiliser ce réseau comme un extracteur de caractéristiques.\n",
    "- Adapter une partie de ce réseau par un petit entrainement. \n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "![source] https://www.researchgate.net/figure/TOP-LEVEL-DIAGRAM-OF-TRANSFER-LEARNING-FROM-A-PRE-TRAINED-CNN-MODEL_fig4_333882146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 : Extraction de caractéristiques "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette technique consiste à utiliser la partie d'extraction de caractéristiques \" `conv_base` \" du CNN. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"  #disables GPU \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2868246527878589329\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6469084774\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 4608829727588648933\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.trainable = False # l'entrainement va pas affecter cette partie\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La dernière couche est une couche de pooling de forme (4,4,512), c'est à partir de cette couche qu'on va coller un un perceptron multicouche pour la classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1 :\n",
    "\n",
    "1- Ajouter une couche de flatten et un perceptron multicouche (à vous de choisir les hyper-paramétres : nombre de couches, nombre de neurones dans chaque couche, fonction d'activation).\n",
    "\n",
    "(indice: \n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(conv_base)\n",
    "\n",
    "model.add(layers.Flatten())\n",
    ")\n",
    "\n",
    "2 - Faire le preprocessing des données mini-caltech101 pour que vos images soient preprocess de la même façon qu'elles l'étaient lors de l'entraînement de VGG16\n",
    "\n",
    "voir https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/preprocess_input, prêter attention à l'input range des images de cette fonction.\n",
    "   \n",
    "3- Faire l'entrainement sur  avec un lr=1e-5 pendant 20 époques.\n",
    "    \n",
    "4- Tracer les courbes de précision et d'erreur et comparer les résultats obtenus avec les résultats du lab\n",
    "précedent. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(conv_base)\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(7* 7,)))\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(7* 7,)))\n",
    "\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(7* 7,)))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_data (data_path,img_shape):\n",
    "    data_dir_list = os.listdir(data_path)\n",
    "    img_rows=img_shape[0]\n",
    "    img_cols=img_shape[1]\n",
    "    num_channel=img_shape[2]\n",
    "\n",
    "\n",
    "    labels=[]\n",
    "    data=[]\n",
    "    len_list_img=0\n",
    "    num_class=0\n",
    "    for dataset in data_dir_list:\n",
    "        img_list=os.listdir(data_path+'/'+ dataset)    \n",
    "        for img in img_list:\n",
    "            input_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\n",
    "            input_img=cv2.resize(input_img,(img_rows,img_cols))\n",
    "            input_img = tf.keras.applications.vgg16.preprocess_input(input_img, data_format=None)\n",
    "            data.append(input_img)\n",
    "            labels.append(num_class)\n",
    "        num_class+=1\n",
    "\n",
    "    data = np.array(data)\n",
    "    data = data.astype('float32')\n",
    "    data /= 255\n",
    "    labels=np.array(labels)\n",
    "    print('data shape',data.shape)\n",
    "    print('labels shape',labels.shape)\n",
    "    return data, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape (500, 224, 224, 3)\n",
      "labels shape (500,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_path='mini_Caltech101'\n",
    "img_shape=[224,224,3]\n",
    "data, labels=load_data (data_path,img_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "num_classes=10\n",
    "Y = to_categorical(labels, num_classes)\n",
    "X_train,y_train = shuffle(data,Y, random_state=2)\n",
    "X_train, X_test, y_train , y_test = train_test_split(X_train,y_train, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD,RMSprop,Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 28,090,698\n",
      "Trainable params: 13,376,010\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sgd = SGD(learning_rate=1e-5, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "24/24 [==============================] - 3s 88ms/step - loss: 2.3537 - accuracy: 0.1343 - val_loss: 2.3257 - val_accuracy: 0.1200\n",
      "Epoch 2/20\n",
      "24/24 [==============================] - 2s 75ms/step - loss: 2.2939 - accuracy: 0.1943 - val_loss: 2.2675 - val_accuracy: 0.2133\n",
      "Epoch 3/20\n",
      "24/24 [==============================] - 2s 77ms/step - loss: 2.2379 - accuracy: 0.2457 - val_loss: 2.2172 - val_accuracy: 0.2600\n",
      "Epoch 4/20\n",
      "24/24 [==============================] - 2s 76ms/step - loss: 2.1881 - accuracy: 0.2857 - val_loss: 2.1744 - val_accuracy: 0.3067\n",
      "Epoch 5/20\n",
      "24/24 [==============================] - 2s 75ms/step - loss: 2.1446 - accuracy: 0.3229 - val_loss: 2.1370 - val_accuracy: 0.3133\n",
      "Epoch 6/20\n",
      "24/24 [==============================] - 2s 75ms/step - loss: 2.1063 - accuracy: 0.3829 - val_loss: 2.1032 - val_accuracy: 0.3800\n",
      "Epoch 7/20\n",
      "24/24 [==============================] - 2s 75ms/step - loss: 2.0709 - accuracy: 0.4114 - val_loss: 2.0719 - val_accuracy: 0.4067\n",
      "Epoch 8/20\n",
      "24/24 [==============================] - 2s 76ms/step - loss: 2.0385 - accuracy: 0.4600 - val_loss: 2.0415 - val_accuracy: 0.4800\n",
      "Epoch 9/20\n",
      "24/24 [==============================] - 2s 75ms/step - loss: 2.0072 - accuracy: 0.4886 - val_loss: 2.0124 - val_accuracy: 0.4933\n",
      "Epoch 10/20\n",
      "24/24 [==============================] - 2s 74ms/step - loss: 1.9786 - accuracy: 0.5029 - val_loss: 1.9839 - val_accuracy: 0.5000\n",
      "Epoch 11/20\n",
      "24/24 [==============================] - 2s 75ms/step - loss: 1.9493 - accuracy: 0.5400 - val_loss: 1.9571 - val_accuracy: 0.5200\n",
      "Epoch 12/20\n",
      "24/24 [==============================] - 2s 76ms/step - loss: 1.9213 - accuracy: 0.5686 - val_loss: 1.9302 - val_accuracy: 0.5400\n",
      "Epoch 13/20\n",
      "24/24 [==============================] - 2s 76ms/step - loss: 1.8923 - accuracy: 0.5914 - val_loss: 1.9027 - val_accuracy: 0.5533\n",
      "Epoch 14/20\n",
      "24/24 [==============================] - 2s 79ms/step - loss: 1.8670 - accuracy: 0.6371 - val_loss: 1.8766 - val_accuracy: 0.5867\n",
      "Epoch 15/20\n",
      "24/24 [==============================] - 2s 77ms/step - loss: 1.8385 - accuracy: 0.6571 - val_loss: 1.8504 - val_accuracy: 0.5867\n",
      "Epoch 16/20\n",
      "24/24 [==============================] - 2s 78ms/step - loss: 1.8125 - accuracy: 0.7057 - val_loss: 1.8257 - val_accuracy: 0.6333\n",
      "Epoch 17/20\n",
      "24/24 [==============================] - 2s 79ms/step - loss: 1.7866 - accuracy: 0.7286 - val_loss: 1.8002 - val_accuracy: 0.6467\n",
      "Epoch 18/20\n",
      "24/24 [==============================] - 2s 79ms/step - loss: 1.7595 - accuracy: 0.7286 - val_loss: 1.7755 - val_accuracy: 0.6533\n",
      "Epoch 19/20\n",
      "24/24 [==============================] - 2s 77ms/step - loss: 1.7335 - accuracy: 0.7543 - val_loss: 1.7507 - val_accuracy: 0.6800\n",
      "Epoch 20/20\n",
      "24/24 [==============================] - 2s 76ms/step - loss: 1.7069 - accuracy: 0.7829 - val_loss: 1.7244 - val_accuracy: 0.7200\n"
     ]
    }
   ],
   "source": [
    "n_epoch=20\n",
    "hist = model.fit(X_train, y_train, batch_size=15, epochs=n_epoch, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 : Adaptation d'un CNN pré-entraîné   \n",
    "\n",
    "Dans cette partie, on va faire un petit réglage pour quelques couches de convolution de notre modèle. Cette méthode ajuste légèrement les représentations les plus abstraites du modèle réutilisé, afin de les rendre plus pertinentes pour le problème en question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Les étapes suivantes résument cet algorithme :\n",
    "\n",
    "1- Ajouter un perceptron multicouche.\n",
    "\n",
    "2- Débloquer quelques couches de la base de convolution. \n",
    "\n",
    "3- Faire l'entrainement. \n",
    "\n",
    "Par exemple, on va débloquer les couches qui suivent `block4_pool` et laisser les autres bloquées. C'est à dire, les couches `block5_conv1`, `block5_conv2` et `block5_conv3` vont être modifiées lors de l'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = models.Sequential()\n",
    "\n",
    "model2.add(conv_base)\n",
    "\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(layers.Dense(512, activation='relu', input_shape=(7* 7,)))\n",
    "model2.add(layers.Dense(512, activation='relu', input_shape=(7* 7,)))\n",
    "\n",
    "model2.add(layers.Dense(512, activation='relu', input_shape=(7* 7,)))\n",
    "model2.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "24/24 [==============================] - 6s 112ms/step - loss: 2.3390 - accuracy: 0.0971 - val_loss: 2.2847 - val_accuracy: 0.1133\n",
      "Epoch 2/20\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 2.2847 - accuracy: 0.1171 - val_loss: 2.2388 - val_accuracy: 0.1267\n",
      "Epoch 3/20\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 2.2300 - accuracy: 0.1486 - val_loss: 2.1929 - val_accuracy: 0.1867\n",
      "Epoch 4/20\n",
      "24/24 [==============================] - 2s 96ms/step - loss: 2.1794 - accuracy: 0.2114 - val_loss: 2.1511 - val_accuracy: 0.2733\n",
      "Epoch 5/20\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 2.1297 - accuracy: 0.3686 - val_loss: 2.1091 - val_accuracy: 0.3800\n",
      "Epoch 6/20\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 2.0835 - accuracy: 0.4657 - val_loss: 2.0658 - val_accuracy: 0.4467\n",
      "Epoch 7/20\n",
      "24/24 [==============================] - 2s 97ms/step - loss: 2.0387 - accuracy: 0.5543 - val_loss: 2.0247 - val_accuracy: 0.5067\n",
      "Epoch 8/20\n",
      "24/24 [==============================] - 2s 97ms/step - loss: 1.9936 - accuracy: 0.6514 - val_loss: 1.9831 - val_accuracy: 0.5667\n",
      "Epoch 9/20\n",
      "24/24 [==============================] - 2s 97ms/step - loss: 1.9497 - accuracy: 0.7086 - val_loss: 1.9416 - val_accuracy: 0.6200\n",
      "Epoch 10/20\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 1.9049 - accuracy: 0.7343 - val_loss: 1.8997 - val_accuracy: 0.6867\n",
      "Epoch 11/20\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 1.8597 - accuracy: 0.7857 - val_loss: 1.8558 - val_accuracy: 0.7333\n",
      "Epoch 12/20\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 1.8144 - accuracy: 0.8200 - val_loss: 1.8110 - val_accuracy: 0.7533\n",
      "Epoch 13/20\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 1.7666 - accuracy: 0.8343 - val_loss: 1.7642 - val_accuracy: 0.7800\n",
      "Epoch 14/20\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 1.7184 - accuracy: 0.8657 - val_loss: 1.7175 - val_accuracy: 0.8133\n",
      "Epoch 15/20\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 1.6695 - accuracy: 0.8886 - val_loss: 1.6692 - val_accuracy: 0.8733\n",
      "Epoch 16/20\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 1.6173 - accuracy: 0.9114 - val_loss: 1.6190 - val_accuracy: 0.9133\n",
      "Epoch 17/20\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 1.5684 - accuracy: 0.9114 - val_loss: 1.5701 - val_accuracy: 0.9200\n",
      "Epoch 18/20\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 1.5156 - accuracy: 0.9257 - val_loss: 1.5199 - val_accuracy: 0.9267\n",
      "Epoch 19/20\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 1.4644 - accuracy: 0.9400 - val_loss: 1.4699 - val_accuracy: 0.9267\n",
      "Epoch 20/20\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 1.4124 - accuracy: 0.9514 - val_loss: 1.4193 - val_accuracy: 0.9400\n"
     ]
    }
   ],
   "source": [
    "n_epoch=20\n",
    "sgd = SGD(learning_rate=1e-5, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model2.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=[\"accuracy\"])\n",
    "\n",
    "hist = model2.fit(X_train, y_train, batch_size=15, epochs=n_epoch, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2:\n",
    "\n",
    "1- Réaliser l'entrainement du CNN pré-entrainé adapté sur mini-caltech101 (lr=1e-5, 20 époques). \n",
    "\n",
    "2- Comparer les résultats obtenus avec les résultats de l'exercice 1. \n",
    "\n",
    "3- Expliquer pourquoi on utilise un lr (learning rate) faible.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 3 : KNN + CNN pré-entrainé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3 : \n",
    "1- Ajouter le VGG entrainé sur ImageNet. (indice : include_top = True)\n",
    "\n",
    "- Pour la bases de données mini-caltech101 :\n",
    "\n",
    "2- Extraire le vecteur de caractéristique de l'avant dernière couche. (vecteur de 4096 valeurs)\n",
    "\n",
    "(indice : \n",
    "get_layer_output= K.function([model.input], [model.layers[layer_idx].output])\n",
    "\n",
    "avec le `num_layer` et le numéro de la couche en considération)\n",
    "\n",
    "\n",
    "3- Utiliser KNN pour classifier les images (K=1). \n",
    "\n",
    "(indice : from sklearn.neighbors import KNeighborsClassifier)\n",
    "\n",
    "4- Comparer les résultats obtenues avec les résultats de l'exercice 1 et 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(weights='imagenet',\n",
    "                  include_top=True,\n",
    "                  input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch=20\n",
    "sgd = SGD(learning_rate=1e-5, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "vgg.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=[\"accuracy\"])\n",
    "get_layer_output= K.function([vgg.input], [vgg.layers[-2].output])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 224, 224, 3) dtype=float32 (created by layer 'input_1')>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 4096) dtype=float32 (created by layer 'fc2')>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg.layers[-2].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = []\n",
    "train_classes = []\n",
    "val_results = []\n",
    "val_classes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train)):\n",
    "    train_results.append(get_layer_output(tf.expand_dims(X_train[i], 0, name=None))[0][0])\n",
    "    train_classes.append(y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_test)):\n",
    "    val_results.append(get_layer_output(tf.expand_dims(X_test[i], 0, name=None))[0][0])\n",
    "    val_classes.append(y_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=1)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "neigh.fit(train_results,train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(neigh.predict([train_results[1]]))\n",
    "print(train_classes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.predict([train_results[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classes[1] == neigh.predict([train_results[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "for i in range(len(val_results)):\n",
    "    if (neigh.predict([val_results[i]])==val_classes[i]).all():\n",
    "        accuracy+=1\n",
    "\n",
    "print(accuracy/len(val_results))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation : \n",
    "\n",
    "Exercice 1 : 30%\n",
    "    \n",
    "Exercice 2 : 30%\n",
    "    \n",
    "Exercice 3 : 40%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
